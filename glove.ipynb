{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glove.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "acPWLY_cZ2sT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import library\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G0sB1nhZ6NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = ['Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque',\n",
        "          'We analyze and make explicit the model properties needed for such regularities to emerge in word vectors','The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCocTj99ta_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "545dd08b-9cd4-4ebb-a392-924f49a411e6"
      },
      "source": [
        "#obtain the list of words.\n",
        "def tokenize_corpus(corpus):\n",
        "    tokens = [x.split() for x in corpus]\n",
        "    return tokens\n",
        "\n",
        "tokenized_corpus = tokenize_corpus(corpus)\n",
        "print(tokenized_corpus)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Recent', 'methods', 'for', 'learning', 'vector', 'space', 'representations', 'of', 'words', 'have', 'succeeded', 'in', 'capturing', 'fine-grained', 'semantic', 'and', 'syntactic', 'regularities', 'using', 'vector', 'arithmetic,', 'but', 'the', 'origin', 'of', 'these', 'regularities', 'has', 'remained', 'opaque'], ['We', 'analyze', 'and', 'make', 'explicit', 'the', 'model', 'properties', 'needed', 'for', 'such', 'regularities', 'to', 'emerge', 'in', 'word', 'vectors'], ['The', 'result', 'is', 'a', 'new', 'global', 'logbilinear', 'regression', 'model', 'that', 'combines', 'the', 'advantages', 'of', 'the', 'two', 'major', 'model', 'families', 'in', 'the', 'literature:', 'global', 'matrix', 'factorization', 'and', 'local', 'context', 'window', 'methods']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqiJjRFnPzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#corpus[2].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGAqQq-OZGGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dictionary and assign the index for words\n",
        "my_dict = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in my_dict:\n",
        "            my_dict.append(token)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(my_dict)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(my_dict)}\n",
        "\n",
        "my_dict_size = len(my_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWP0bhRtwXp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d41992e7-aa11-4465-c7e1-e252d92d60ad"
      },
      "source": [
        "word2idx"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Recent': 0,\n",
              " 'The': 39,\n",
              " 'We': 27,\n",
              " 'a': 42,\n",
              " 'advantages': 49,\n",
              " 'analyze': 28,\n",
              " 'and': 15,\n",
              " 'arithmetic,': 19,\n",
              " 'but': 20,\n",
              " 'capturing': 12,\n",
              " 'combines': 48,\n",
              " 'context': 57,\n",
              " 'emerge': 36,\n",
              " 'explicit': 30,\n",
              " 'factorization': 55,\n",
              " 'families': 52,\n",
              " 'fine-grained': 13,\n",
              " 'for': 2,\n",
              " 'global': 44,\n",
              " 'has': 24,\n",
              " 'have': 9,\n",
              " 'in': 11,\n",
              " 'is': 41,\n",
              " 'learning': 3,\n",
              " 'literature:': 53,\n",
              " 'local': 56,\n",
              " 'logbilinear': 45,\n",
              " 'major': 51,\n",
              " 'make': 29,\n",
              " 'matrix': 54,\n",
              " 'methods': 1,\n",
              " 'model': 31,\n",
              " 'needed': 33,\n",
              " 'new': 43,\n",
              " 'of': 7,\n",
              " 'opaque': 26,\n",
              " 'origin': 22,\n",
              " 'properties': 32,\n",
              " 'regression': 46,\n",
              " 'regularities': 17,\n",
              " 'remained': 25,\n",
              " 'representations': 6,\n",
              " 'result': 40,\n",
              " 'semantic': 14,\n",
              " 'space': 5,\n",
              " 'succeeded': 10,\n",
              " 'such': 34,\n",
              " 'syntactic': 16,\n",
              " 'that': 47,\n",
              " 'the': 21,\n",
              " 'these': 23,\n",
              " 'to': 35,\n",
              " 'two': 50,\n",
              " 'using': 18,\n",
              " 'vector': 4,\n",
              " 'vectors': 38,\n",
              " 'window': 58,\n",
              " 'word': 37,\n",
              " 'words': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz9y13PoaoMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 2\n",
        "idx_pairs = []\n",
        "# for each sentence\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "    # for each word, threated as center word\n",
        "    for center_word_pos in range(len(indices)):\n",
        "        # for each window position\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            # make soure not jump out sentence\n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLL4_wBva5Wk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "68442dda-d307-45c5-88ca-a784e6d4ac4d"
      },
      "source": [
        "idx_pairs[:10]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [0, 2],\n",
              "       [1, 0],\n",
              "       [1, 2],\n",
              "       [1, 3],\n",
              "       [2, 0],\n",
              "       [2, 1],\n",
              "       [2, 3],\n",
              "       [2, 4],\n",
              "       [3, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzmbXSEroEEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_layer(word_idx):\n",
        "    x = torch.zeros(my_dict_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "  \n",
        "  #Input layer is just the center word encoded in one-hot manner. It dimensions are [1, vocabulary_size]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKiYTsqmoIey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47f781a3-17d5-40b4-c480-9173fa2a7b14"
      },
      "source": [
        "embedding_dims = 5\n",
        "W1 = Variable(torch.randn(embedding_dims, my_dict_size).float(), requires_grad=True)\n",
        "W2 = Variable(torch.randn(my_dict_size, embedding_dims).float(), requires_grad=True)\n",
        "num_epochs = 1010\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epo in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for data, target in idx_pairs:\n",
        "        x = Variable(get_input_layer(data)).float()\n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "\n",
        "        z1 = torch.matmul(W1, x)\n",
        "        z2 = torch.matmul(W2, z1)\n",
        "    \n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "\n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        loss_val += loss.data.item()\n",
        "        loss.backward()\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "    if epo % 10 == 0:    \n",
        "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epo 0: 6.15251525599381\n",
            "Loss at epo 10: 5.709854064316585\n",
            "Loss at epo 20: 5.389699952766813\n",
            "Loss at epo 30: 5.143794337223316\n",
            "Loss at epo 40: 4.947761688561275\n",
            "Loss at epo 50: 4.787342763769216\n",
            "Loss at epo 60: 4.653403744204291\n",
            "Loss at epo 70: 4.539722133093867\n",
            "Loss at epo 80: 4.441879593914953\n",
            "Loss at epo 90: 4.356644013010222\n",
            "Loss at epo 100: 4.281588464358757\n",
            "Loss at epo 110: 4.214856257931939\n",
            "Loss at epo 120: 4.154997875361607\n",
            "Loss at epo 130: 4.1008699223913\n",
            "Loss at epo 140: 4.051554217009709\n",
            "Loss at epo 150: 4.006308357468967\n",
            "Loss at epo 160: 3.9645252897821623\n",
            "Loss at epo 170: 3.925702892500779\n",
            "Loss at epo 180: 3.8894254766661547\n",
            "Loss at epo 190: 3.8553448245443147\n",
            "Loss at epo 200: 3.8231700231289043\n",
            "Loss at epo 210: 3.792655052398813\n",
            "Loss at epo 220: 3.7635921375504857\n",
            "Loss at epo 230: 3.735805899932467\n",
            "Loss at epo 240: 3.7091466747481245\n",
            "Loss at epo 250: 3.6834885806872926\n",
            "Loss at epo 260: 3.658723433675437\n",
            "Loss at epo 270: 3.6347598639027825\n",
            "Loss at epo 280: 3.6115198727311757\n",
            "Loss at epo 290: 3.588936067860702\n",
            "Loss at epo 300: 3.566951651819821\n",
            "Loss at epo 310: 3.545517220990411\n",
            "Loss at epo 320: 3.524590160517857\n",
            "Loss at epo 330: 3.504133170637591\n",
            "Loss at epo 340: 3.484115187875156\n",
            "Loss at epo 350: 3.4645087651137647\n",
            "Loss at epo 360: 3.445289985270336\n",
            "Loss at epo 370: 3.4264384261493026\n",
            "Loss at epo 380: 3.4079363337878523\n",
            "Loss at epo 390: 3.3897701621055605\n",
            "Loss at epo 400: 3.371927488672322\n",
            "Loss at epo 410: 3.3543975474505587\n",
            "Loss at epo 420: 3.3371715465496328\n",
            "Loss at epo 430: 3.320242469064121\n",
            "Loss at epo 440: 3.303603937297032\n",
            "Loss at epo 450: 3.2872517480932433\n",
            "Loss at epo 460: 3.2711807978564296\n",
            "Loss at epo 470: 3.255388303460746\n",
            "Loss at epo 480: 3.239870495631777\n",
            "Loss at epo 490: 3.2246252481279702\n",
            "Loss at epo 500: 3.2096499584872147\n",
            "Loss at epo 510: 3.1949421767530772\n",
            "Loss at epo 520: 3.180499152068434\n",
            "Loss at epo 530: 3.1663185929429942\n",
            "Loss at epo 540: 3.152398053325456\n",
            "Loss at epo 550: 3.1387335312777553\n",
            "Loss at epo 560: 3.12532215611688\n",
            "Loss at epo 570: 3.112160199058467\n",
            "Loss at epo 580: 3.099242727098794\n",
            "Loss at epo 590: 3.0865646312976707\n",
            "Loss at epo 600: 3.07412111142586\n",
            "Loss at epo 610: 3.0619067237294955\n",
            "Loss at epo 620: 3.0499157188267545\n",
            "Loss at epo 630: 3.0381418158268105\n",
            "Loss at epo 640: 3.0265790269292636\n",
            "Loss at epo 650: 3.0152213160333963\n",
            "Loss at epo 660: 3.0040626338843643\n",
            "Loss at epo 670: 2.99309673802606\n",
            "Loss at epo 680: 2.982318381399944\n",
            "Loss at epo 690: 2.9717216993200366\n",
            "Loss at epo 700: 2.9613010427047466\n",
            "Loss at epo 710: 2.9510518308343556\n",
            "Loss at epo 720: 2.940968659006316\n",
            "Loss at epo 730: 2.9310466371733566\n",
            "Loss at epo 740: 2.921281268267796\n",
            "Loss at epo 750: 2.911668904485374\n",
            "Loss at epo 760: 2.9022050210114183\n",
            "Loss at epo 770: 2.8928854529199928\n",
            "Loss at epo 780: 2.883706460533471\n",
            "Loss at epo 790: 2.8746647316834024\n",
            "Loss at epo 800: 2.8657565969845344\n",
            "Loss at epo 810: 2.8569781457555705\n",
            "Loss at epo 820: 2.8483261568792937\n",
            "Loss at epo 830: 2.8397977471351625\n",
            "Loss at epo 840: 2.8313898361962417\n",
            "Loss at epo 850: 2.8230987382346187\n",
            "Loss at epo 860: 2.8149220719419676\n",
            "Loss at epo 870: 2.806856499047115\n",
            "Loss at epo 880: 2.7988997516960934\n",
            "Loss at epo 890: 2.791048945229629\n",
            "Loss at epo 900: 2.7833013039210748\n",
            "Loss at epo 910: 2.7756541389843514\n",
            "Loss at epo 920: 2.7681053679564904\n",
            "Loss at epo 930: 2.76065249052541\n",
            "Loss at epo 940: 2.7532930900310646\n",
            "Loss at epo 950: 2.7460253818281766\n",
            "Loss at epo 960: 2.738847378410142\n",
            "Loss at epo 970: 2.731756832681853\n",
            "Loss at epo 980: 2.7247519735632273\n",
            "Loss at epo 990: 2.7178306651526483\n",
            "Loss at epo 1000: 2.710991484543373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKVIa7C5o_HX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca35831c-50c3-4be9-86f1-7c110138805c"
      },
      "source": [
        "#check the index of word\n",
        "for ind, token in enumerate(my_dict):\n",
        "  word2idx[token] = ind\n",
        "  idx2word[ind] = token\n",
        "print(word2idx['methods'])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diNmnW4K3Lz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a zero matrix with size of length dictionary\n",
        "length=len(my_dict)\n",
        "cooc = np.zeros([length,length], np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Kqgauo-3rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a function that loops over the words on a sentence and updates the co-ocurrence matrix\n",
        "def process_sentence(sentence):\n",
        "    words_in_sentence = word2idx\n",
        "    list_of_indeces = [my_dict.index(word) for word in words_in_sentence]\n",
        "    for index1 in list_of_indeces:\n",
        "        for index2 in list_of_indeces:\n",
        "            if index1 != index2:\n",
        "                cooc[index1,index2] +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU-hZvLq_Ezh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#go through all\n",
        "for sentence in my_dict:\n",
        "    process_sentence(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlq0e90t5cXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3159c58c-cd7d-4f68-f217-be1e487a248e"
      },
      "source": [
        "print(cooc)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0. 59. 59. ... 59. 59. 59.]\n",
            " [59.  0. 59. ... 59. 59. 59.]\n",
            " [59. 59.  0. ... 59. 59. 59.]\n",
            " ...\n",
            " [59. 59. 59. ...  0. 59. 59.]\n",
            " [59. 59. 59. ... 59.  0. 59.]\n",
            " [59. 59. 59. ... 59. 59.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u950vw6BlHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coocs = np.transpose(np.nonzero(cooc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHX7HSpLElIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "73ebfcb1-6467-4fd6-b091-0df8d2e02b87"
      },
      "source": [
        "print(coocs)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1]\n",
            " [ 0  2]\n",
            " [ 0  3]\n",
            " ...\n",
            " [58 55]\n",
            " [58 56]\n",
            " [58 57]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGDpmaECEnvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_func(x, x_max, alpha):\n",
        "    wx = (x/x_max)**alpha\n",
        "    wx = torch.min(wx, torch.ones_like(wx))\n",
        "    return wx.cuda()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWmUKA7QFF7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}